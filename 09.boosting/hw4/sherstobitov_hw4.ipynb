{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 10 мая 2021, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 10 мая, -4 балла после 08:30 17 мая, -6 баллов после 08:30 24 мая, -8 баллов после 08:30 31 мая.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0221, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше решение тут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MSE}::-\\frac{d}{da}L = 2 (y_i - a(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Экспоненциальная}::-\\frac{d}{da}L = y_i \\exp(-a(x_i)y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Логистическая}::-\\frac{d}{da}L = \\frac{y_i \\exp(-a(x_i)y_i)}{1 + \\exp(-a(x_i)y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss='MSE', learning_rate=0.1, n_estimators=100,\n",
    "                 colsample=1.0, subsample=1.0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        subsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        if loss == 'MSE':\n",
    "            self.__get_loss = self.__get_MSE\n",
    "            self.__get_grad = self.__get_MSE_grad\n",
    "        elif loss == 'exp':\n",
    "            self.__get_loss = self.__get_exp\n",
    "            self.__get_grad = self.__get_exp_grad\n",
    "        else:\n",
    "            self.__get_loss = self.__get_log\n",
    "            self.__get_grad = self.__get_log_grad\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.attr_fract = colsample\n",
    "        self.sample_fract = subsample\n",
    "        self.noname_params = args\n",
    "        self.withname_params = kwargs\n",
    "\n",
    "    def __get_MSE(self, real, preds):\n",
    "        return (real - preds) ** 2\n",
    "\n",
    "    def __get_exp(self, real, preds):\n",
    "        return np.exp(-preds * real)\n",
    "\n",
    "    def __get_log(self, real, preds):\n",
    "        return np.log(1 + np.exp(-preds * real))\n",
    "\n",
    "    def __get_MSE_grad(self, real, preds):\n",
    "        return 2 * (real - preds)\n",
    "\n",
    "    def __get_exp_grad(self, real, preds):\n",
    "        return real * np.exp(-preds * real)\n",
    "\n",
    "    def __get_log_grad(self, real, preds):\n",
    "        return real * np.exp(-preds * real) / (1 + np.exp(-preds * real))\n",
    "\n",
    "    def __init_model(self, init_model):\n",
    "        if init_model is None:\n",
    "            self.booster = np.float64(np.ones(self.y.shape[0]) / self.y.shape[0])\n",
    "            self.init_model = None\n",
    "        else:\n",
    "            self.init_model = init_model(*self.noname_params, **self.withname_params)\n",
    "            self.init_model.fit(self.X, self.y)\n",
    "            self.booster = np.float64(self.init_model.predict(self.X))\n",
    "            \n",
    "    def __choose_data(self):\n",
    "        features_inds = np.random.randint(\n",
    "            0, self.X.shape[1], size=int(self.X.shape[1] * self.attr_fract))\n",
    "        self.features_inds.append(features_inds)\n",
    "        objects_inds = np.random.randint(\n",
    "            0, self.X.shape[0], size=int(self.X.shape[0] * self.sample_fract))\n",
    "        X_algo = self.X[np.ix_(objects_inds, features_inds)]\n",
    "        X_pred = self.X[:, features_inds]\n",
    "        y_algo = self.y[objects_inds]\n",
    "        prev_res = self.booster[objects_inds]\n",
    "        return X_algo, X_pred, y_algo, prev_res\n",
    "\n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.__init_model(init_model)\n",
    "\n",
    "        self.models = []\n",
    "        self.features_inds = []\n",
    "        for estimator in range(self.n_estimators):\n",
    "            X_algo, X_pred, y_algo, prev_res = self.__choose_data()\n",
    "            target = self.__get_grad(y_algo, prev_res)\n",
    "            model = base_model(*self.noname_params, **self.withname_params)\n",
    "            model.fit(X_algo, target)\n",
    "            self.models.append(model)\n",
    "            self.booster += self.learning_rate * \\\n",
    "                model.predict(X_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.init_model is None:\n",
    "            preds = np.float64(np.ones(X.shape[0]) / X.shape[0])\n",
    "        else:\n",
    "            preds = np.float64(self.init_model.predict(X))\n",
    "        for model, feat in zip(self.models, self.features_inds):\n",
    "            preds += self.learning_rate * model.predict(X[:, feat])\n",
    "        return np.around(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValScore(n_splits=5, *args, **kwargs):\n",
    "    clf = MyGradientBoostingClassifier(*args, **kwargs)\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train, test in kf.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        scaler.transform(X_train)\n",
    "        scaler.transform(X_test)\n",
    "        clf.fit(X_train, y_train, base_model=RandomForestRegressor)\n",
    "        scores.append(f1_score(y_pred=clf.predict(X_test),\n",
    "                               y_true=y_test))\n",
    "    return np.asarray(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindParams(param_name, param_range, known_params=opt_params):\n",
    "    mean_scores = []\n",
    "\n",
    "    for param in param_range:\n",
    "        kwargs = known_params\n",
    "        kwargs.update({param_name: param})\n",
    "        scores = ValScore(**kwargs)\n",
    "        mean_scores.append(scores.mean())\n",
    "\n",
    "    opt_param = param_range[np.argmax(mean_scores)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('score')\n",
    "    plt.title(f'Зависимость f1-score от параметра {param_name}\\n'\n",
    "              f'Оптимальное значение параметра {param_name}: {opt_param}')\n",
    "    plt.plot(param_range, mean_scores)\n",
    "\n",
    "    return opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(20, 161, 20)\n",
    "opt_param = FindParams('n_estimators', estimators)\n",
    "opt_params.update({'n_estimators': opt_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['MSE', 'exp', 'log']\n",
    "opt_param = FindParams('loss', losses, opt_params)\n",
    "opt_params.update({'loss': opt_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = [0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "opt_param = FindParams('learning_rate', rates, opt_params)\n",
    "opt_params.update({'learning_rate': opt_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsamples = [1.0, 0.7, 0.5, 0.2]\n",
    "opt_param = FindParams('colsample', colsamples, opt_params)\n",
    "opt_params.update({'colsample': opt_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [1.0, 0.7, 0.5, 0.2]\n",
    "opt_param = FindParams('subsample', colsamples, opt_params)\n",
    "opt_params.update({'subsample': opt_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier(**opt_params)\n",
    "model.fit(X_train, y_train)\n",
    "f1_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier(**opt_params)\n",
    "model.fit(X_train, y_train, base_model=RandomForestRegressor)\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_random_boost(i):\n",
    "    model = MyGradientBoostingClassifier(**opt_params)\n",
    "    samples_inds = np.random.randint(0, X_train.shape[0], size=int(X_train.shape[0] / N))\n",
    "    X_boost = X_train[samples_inds, :]\n",
    "    y_boost = y_train[samples_inds]\n",
    "    model.fit(X_boost, y_boost)\n",
    "    preds = model.predict(X_test)\n",
    "    with lock:\n",
    "        boots_res.append(preds)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 70\n",
    "with Pool(10) as pool, tqdm(N) as pbar:\n",
    "    boots_res = []\n",
    "    lock = pbar.get_lock()\n",
    "    pool.map(process_random_boost, range(N))\n",
    "pool.join()\n",
    "print(accuracy_score(y_test, np.sum(boots_res, axis=0) / N > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier()\n",
    "model.fit(X_train, y_train, init_model=SGDClassifier)\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier()\n",
    "model.fit(X_train, y_train, init_model=SVC)\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier()\n",
    "model.fit(X_train, y_train, init_model=RandomForestRegressor)\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGradientBoostingClassifier()\n",
    "model.fit(X_train, y_train, init_model=RandomForestClassifier)\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, результат улучшился. Думаю потому, что лес выделил удачное направление с самого начала"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('da': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd0fb1e9936674d641b0e1ac5c75cd75c2e7215014ff333cadbd6ad2f998d1f79a6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
